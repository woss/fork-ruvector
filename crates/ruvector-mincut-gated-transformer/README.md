# Mincut-Gated Transformer

> **A novel architecture for ultra-low latency transformer inference combining minimum cut coherence gating with state-of-the-art optimization techniques**

[![Crates.io](https://img.shields.io/crates/v/ruvector-mincut-gated-transformer.svg)](https://crates.io/crates/ruvector-mincut-gated-transformer)
[![Documentation](https://docs.rs/ruvector-mincut-gated-transformer/badge.svg)](https://docs.rs/ruvector-mincut-gated-transformer)
[![License](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](LICENSE)

## Introduction

The **Mincut-Gated Transformer** introduces a novel approach to transformer inference optimization by leveraging minimum cut (mincut) values from attention graphs as coherence signals. Unlike traditional learned gating mechanisms, our approach uses graph-theoretic properties to make deterministic, explainable decisions about compute allocation.

### Key Innovations

- **Î»-based Mixture-of-Depths**: Uses mincut Î»-delta as routing signal instead of learned routers (50% FLOPs reduction)
- **Coherence-driven Early Exit**: Leverages Î» stability for self-speculative decoding (30-50% latency reduction)
- **Mincut Sparse Attention**: Partition boundaries define sparse masks (90% attention FLOPs reduction)
- **Energy-based Gating**: Treats coherence as energy function for principled decisions
- **Spike-driven Scheduling**: Event-driven compute with 87Ã— energy efficiency gains
- **Spectral Position Encoding**: Graph Laplacian eigenvectors for structural awareness

## Features

- **ðŸŽ¯ Deterministic inference** - Same inputs always produce same outputs
- **âš¡ Bounded latency** - Predictable p99 guarantees with tier-based execution
- **ðŸ“Š Explainable decisions** - Every inference produces a witness explaining interventions
- **ðŸ”‹ Energy efficient** - Event-driven spike scheduling and adaptive compute
- **ðŸ’¾ Allocation-free hot path** - Zero heap allocations after initialization
- **ðŸ›¡ï¸ Safety controls** - Coherence-gated state updates prevent contamination

## Academic Foundations

This crate integrates multiple state-of-the-art transformer optimization techniques:

1. **Mixture-of-Depths** (Raposo et al., 2024) - 50% FLOPs reduction through dynamic compute allocation
2. **Early Exit / Self-Speculative Decoding** (Elhoushi et al., 2024) - 30-50% latency reduction
3. **Dynamic Sparse Attention** (Jiang et al., 2024) - 90% attention FLOPs reduction for long contexts
4. **Energy-Based Transformers** (Gladstone et al., 2025) - Principled compute-quality tradeoffs
5. **Spike-Driven Inference** (Yao et al., 2023, 2024) - 87Ã— energy reduction with event-driven compute
6. **Spectral Attention** (Kreuzer et al., 2021) - Graph-based coherence metrics

See [docs/THEORY.md](docs/THEORY.md) for detailed theoretical foundations and [docs/BENCHMARKS.md](docs/BENCHMARKS.md) for performance analysis.

## Quick Start

```rust
use ruvector_mincut_gated_transformer::prelude::*;

// Create configuration
let config = TransformerConfig::micro();
let policy = GatePolicy::default();

// Load weights (or use empty for testing)
let weights = QuantizedWeights::empty(&config);

// Create transformer
let mut transformer = MincutGatedTransformer::new(config, policy, weights)?;

// Create gate packet from mincut signals
let gate = GatePacket {
    lambda: 100,              // Minimum cut value
    lambda_prev: 95,          // Previous lambda
    boundary_edges: 5,        // Cross-partition edges
    boundary_concentration_q15: 8192,  // ~25% concentration
    partition_count: 3,       // Detected partitions
    flags: 0,
};

// Prepare input
let input = InferInput::from_tokens(&[1, 2, 3, 4], gate);

// Allocate output buffer
let mut logits = vec![0i32; config.logits as usize];
let mut output = InferOutput::new(&mut logits);

// Run inference
transformer.infer(&input, &mut output)?;

// Check witness for decisions
println!("Decision: {:?}", output.witness.decision);
println!("Reason: {:?}", output.witness.reason);
println!("External writes allowed: {}", output.witness.external_writes_enabled);

// Check stats
println!("Tier: {}", output.stats.tier);
println!("Layers executed: {}", output.stats.layers_executed);
println!("Effective sequence length: {}", output.stats.effective_seq_len);
```

## Architecture Overview

```
Input â†’ [Spike Scheduler] â†’ [Gate Controller] â†’ [Transformer] â†’ Output + Witness
           â†“                      â†“                    â†“
      Event-driven          Coherence-gated      Adaptive-depth
      Skip/Run              Tier Selection       Early Exit
      decision              KV Flush/Freeze      Sparse Attention
```

### Tier System

| Tier | Layers | Seq Len | Window | Use Case | Expected Speedup |
|------|--------|---------|---------|----------|------------------|
| 0    | 4      | 64      | 16      | Normal   | 1Ã— (baseline) |
| 1    | 2      | 32      | 8       | Reduced  | 2-3Ã— |
| 2    | 1      | 8       | 4       | Safe     | 5-10Ã— |
| 3    | 0      | 0       | 0       | Skip     | 50-200Ã— |

The system automatically selects tiers based on:
- **Coherence metrics:** Lambda (Î»), boundary edges, partition drift
- **Spike signals:** Firing rate, novelty, sparse masks
- **Policy configuration:** Thresholds and safety requirements

## Configuration

### Preset Configurations

```rust
// Micro configuration (WASM, edge gateways)
let config = TransformerConfig::micro();
// - Sequence: 32, Hidden: 128, Heads: 4, Layers: 2

// Baseline configuration (CPU)
let config = TransformerConfig::baseline();
// - Sequence: 64, Hidden: 256, Heads: 4, Layers: 4
```

### Gate Policy

```rust
let policy = GatePolicy {
    lambda_min: 30,                         // Minimum coherence threshold
    drop_ratio_q15_max: 16384,              // Max lambda drop (50%)
    boundary_edges_max: 20,                 // Max cross-partition edges
    boundary_concentration_q15_max: 24576,  // Max concentration (75%)
    partitions_max: 8,                      // Max partition count
    spike_rate_q15_max: 26214,              // Max spike rate (80%)
    allow_kv_write_when_unstable: false,    // Freeze KV on instability
    allow_external_write_when_unstable: false, // Freeze external writes
};
```

## Event-Driven Inference with Spikes

```rust
// Create spike packet
let spike = SpikePacket {
    fired: 1,              // Spike fired
    rate_q15: 16384,       // 50% firing rate
    novelty_q15: 12288,    // 37.5% novelty
    top_len: 3,            // 3 important positions
    top_idx: [5, 10, 15, 0, /* ... */],  // Position indices
    top_w_q15: [16384, 8192, 4096, 0, /* ... */],  // Weights
    flags: SpikePacket::FLAG_SPARSE_MASK,
};

let input = InferInput {
    tokens: Some(&[1, 2, 3, 4]),
    embedding_q: None,
    embedding_scale: 1.0,
    input_signature: None,
    gate,
    spikes: Some(spike),
};

transformer.infer(&input, &mut output)?;
```

When `spike.fired == 0`, inference is completely skipped (tier 3).

## Features

### Core Features

- `sliding_window` (default) - Sliding window attention
- `linear_attention` - Linear attention for longer sequences

### Optimization Features

- `simd` - SIMD-optimized kernels
- `int4` - INT4 quantization support
- `fixed_point_softmax` - Fixed-point softmax for embedded targets
- `rmsnorm` - RMSNorm instead of LayerNorm

### Debugging

- `trace` - Enable tracing and snapshot support

### Platform Support

- `wasm` - WebAssembly support
- `no_std_gateway` - No-std mode for embedded gateways

## Performance

Expected speedups on typical workloads:

| Workload Type | Skip Rate | Expected Speedup | Memory Reduction |
|---------------|-----------|------------------|------------------|
| Streaming (low activity) | 70% | **10-15Ã—** | 80% |
| Interactive (bursty) | 40% | **4-6Ã—** | 50% |
| Continuous (high throughput) | 10% | **2-3Ã—** | 40% |
| Safety-critical (conservative) | 5% | **1.5-2Ã—** | 25% |

See [docs/BENCHMARKS.md](docs/BENCHMARKS.md) for detailed analysis.

## Integration with RuVector

```rust
use ruvector_mincut_gated_transformer::prelude::*;
use ruvector_mincut::MincutEngine;

// Compute mincut signals from attention graph
let mut mincut = MincutEngine::new(num_nodes);
// ... build graph from attention weights ...
let lambda = mincut.compute_mincut();

// Create gate packet
let gate = GatePacket {
    lambda,
    lambda_prev: prev_lambda,
    boundary_edges: mincut.boundary_edge_count(),
    // ... other metrics ...
    ..Default::default()
};

// Use gate for inference
let input = InferInput::from_tokens(tokens, gate);
transformer.infer(&input, &mut output)?;
```

## Safety and Determinism

### Determinism Guarantee

For fixed weights, configuration, policy, and input `(tokens, gate, spikes)`, inference always produces identical `(logits, witness)`.

**No randomness:** Fixed-point arithmetic, deterministic control flow

**No allocations:** Hot path is allocation-free

**Reproducible:** Bit-exact results across runs

### Safety Properties

**External writes enabled only when:**
- `lambda >= lambda_min`
- `drop_ratio < drop_ratio_q15_max`

**KV cache writes controlled:**
- Flushed on coherence loss
- Frozen in tier 2
- Disabled in tier 3

**Witness provides proof:**
- Which interventions occurred
- Why they were triggered
- What state changes were allowed

## Documentation

- **[Theory & Foundations](docs/THEORY.md)** - Academic background and theoretical analysis
- **[Performance Benchmarks](docs/BENCHMARKS.md)** - Expected gains and empirical results
- **[API Documentation](https://docs.rs/ruvector-mincut-gated-transformer)** - Complete API reference

## License

Licensed under either of:

- Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)
- MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)

at your option.

## Academic References

This implementation draws from the following peer-reviewed research:

### Core Optimization Techniques

1. **Raposo, D., et al.** (2024). "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models." *arXiv:2404.02258*.
   - Foundation for Î»-based token routing and layer skipping

2. **Elhoushi, M., et al.** (2024). "LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding." *arXiv:2404.16710*.
   - Basis for coherence-driven early exit mechanism

3. **Jiang, H., et al.** (2024). "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention." *NeurIPS 2024*.
   - Inspiration for mincut-based sparse attention patterns

### Energy and Efficiency

4. **Gladstone, A., et al.** (2025). "Energy-Based Transformers are Scalable Learners and Thinkers." *arXiv:2507.02092*.
   - Theoretical foundation for energy-based gate decisions

5. **Yao, M., et al.** (2023). "Spike-driven Transformer." *NeurIPS 2023*.
   - Event-driven attention with 87Ã— energy reduction

6. **Yao, M., et al.** (2024). "Spike-driven Transformer V2: Meta Spiking Neural Network Architecture." *ICLR 2024*.
   - Advanced spike-driven attention mechanisms

### Graph-Based Methods

7. **Kreuzer, D., et al.** (2021). "Rethinking Graph Transformers with Spectral Attention." *NeurIPS 2021*.
   - Spectral position encoding and graph-based coherence

8. **Vaswani, A., et al.** (2017). "Attention is All You Need." *NeurIPS 2017*.
   - Foundational transformer architecture

9. **VeliÄkoviÄ‡, P., et al.** (2018). "Graph Attention Networks." *ICLR 2018*.
   - Graph-based attention mechanisms

### Quantization

10. **Jacob, B., et al.** (2018). "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference." *CVPR 2018*.
    - INT8 quantization techniques used throughout

For the complete BibTeX citations, see [docs/CITATIONS.bib](docs/CITATIONS.bib).
